{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9a3c105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openpyxl in /home/vscode/.local/lib/python3.11/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /home/vscode/.local/lib/python3.11/site-packages (from openpyxl) (2.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in /home/vscode/.local/lib/python3.11/site-packages (4.14.3)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in /home/vscode/.local/lib/python3.11/site-packages (from beautifulsoup4) (2.8.3)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/vscode/.local/lib/python3.11/site-packages (from beautifulsoup4) (4.15.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd \n",
    "import io\n",
    "import re\n",
    "!pip install openpyxl\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f23de877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most recent dataset link: https://www.ons.gov.uk/file?uri=/economy/economicoutputandproductivity/output/datasets/ukretailfootfall/12february2026/ukretailfootfalldataset120226.xlsx\n",
      "Sheets found: Cover,\n",
      " Contents,\n",
      " Notes,\n",
      " 1.Weekly by region,\n",
      " 2.Weekly by site type,\n",
      " 3.Weekly by region and site,\n",
      " 4.Monthly by region,\n",
      " 5.Monthly by site type,\n",
      " 6.Monthly by region and site\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2026-02-12'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_latest_link():\n",
    "    url = \"https://www.ons.gov.uk/economy/economicoutputandproductivity/output/datasets/ukretailfootfall\"\n",
    "    response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Find all links to .xlsx files\n",
    "    links = soup.find_all(\"a\", href=True)\n",
    "    excel_links = [a['href'] for a in links if a['href'].endswith('.xlsx')]\n",
    "\n",
    "    # The most recent is usually the first one\n",
    "    if excel_links:\n",
    "        latest_link = \"https://www.ons.gov.uk\" + excel_links[0]\n",
    "        print(\"Most recent dataset link:\", latest_link)\n",
    "        return latest_link\n",
    "    else:\n",
    "        print(\"No Excel links found.\")\n",
    "        return ''\n",
    "    \n",
    "\n",
    "def get_excel(url):\n",
    "    try:\n",
    "        # Download into memory\n",
    "        resp = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=60)\n",
    "        resp.raise_for_status()  # raises HTTPError for 4xx/5xx\n",
    "\n",
    "        # Optional: lightweight content-type check (won't always be accurate)\n",
    "        ctype = resp.headers.get('Content-Type', '')\n",
    "        if 'excel' not in ctype and 'spreadsheetml' not in ctype:\n",
    "            print(f\"Warning: unexpected content type: {ctype}\")\n",
    "\n",
    "        # Read the entire workbook: sheet_name=None returns a dict of DataFrames\n",
    "        xls = pd.read_excel(io.BytesIO(resp.content), sheet_name=None, engine='openpyxl')\n",
    "\n",
    "        # Output the titles (sheet names)\n",
    "        print(\"Sheets found:\", \",\\n \".join(xls.keys()))\n",
    "\n",
    "        # Example: preview each sheet\n",
    "        # for name, df in xls.items():\n",
    "        #     print(f\"\\n=== Sheet: {name} ===\")\n",
    "        #     print(df.head())\n",
    "        \n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"Error: download timed out.\")\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP error: {e}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Network error: {e}\")\n",
    "    except ValueError as e:\n",
    "        # pandas read_excel errors (e.g., corrupted file)\n",
    "        print(f\"Excel parse error: {e}\")\n",
    "    return xls\n",
    "\n",
    "def get_version(xls):\n",
    "    version = \"\"\n",
    "    try:\n",
    "        raw = xls['Cover'].iloc[3,0]  # A5 -> row index 4, column 0\n",
    "\n",
    "        m = re.search(r\"(\\d{1,2}\\s+[A-Za-z]+\\s+\\d{4})\", raw)\n",
    "        if m:       \n",
    "            version = pd.to_datetime(m.group(1), dayfirst=True).strftime('%Y-%m-%d')\n",
    "        else:\n",
    "            # fallback: try to coerce any parsable date inside the string\n",
    "            parsed = pd.to_datetime(raw, dayfirst=True, errors='coerce')\n",
    "            version = parsed.strftime('%Y-%m-%d') if not pd.isna(parsed) else \"\"\n",
    "    except Exception as e:\n",
    "        print('Could not extract publication date from Cover sheet:', e)\n",
    "        version = \"\"\n",
    "    return version\n",
    "\n",
    " \n",
    "xls = get_excel(get_latest_link()) # display(df.head())  # or print(df.head()) if not in a notebook\n",
    "version = get_version(xls)\n",
    "\n",
    "version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddffc2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.Weekly by region',\n",
       " '2.Weekly by site type',\n",
       " '3.Weekly by region and site',\n",
       " '4.Monthly by region',\n",
       " '5.Monthly by site type',\n",
       " '6.Monthly by region and site']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85aff4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period_end_dt</th>\n",
       "      <th>region</th>\n",
       "      <th>footfall_index</th>\n",
       "      <th>site_type</th>\n",
       "      <th>period_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-07-07</td>\n",
       "      <td>UK total</td>\n",
       "      <td>100.640079</td>\n",
       "      <td>all</td>\n",
       "      <td>week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-07-07</td>\n",
       "      <td>England</td>\n",
       "      <td>100.894023</td>\n",
       "      <td>all</td>\n",
       "      <td>week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-07-07</td>\n",
       "      <td>Northern Ireland</td>\n",
       "      <td>109.402150</td>\n",
       "      <td>all</td>\n",
       "      <td>week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-07-07</td>\n",
       "      <td>Scotland</td>\n",
       "      <td>96.063476</td>\n",
       "      <td>all</td>\n",
       "      <td>week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-07-07</td>\n",
       "      <td>Wales</td>\n",
       "      <td>99.099980</td>\n",
       "      <td>all</td>\n",
       "      <td>week</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  period_end_dt            region  footfall_index site_type period_type\n",
       "0    2024-07-07          UK total      100.640079       all        week\n",
       "1    2024-07-07           England      100.894023       all        week\n",
       "2    2024-07-07  Northern Ireland      109.402150       all        week\n",
       "3    2024-07-07          Scotland       96.063476       all        week\n",
       "4    2024-07-07             Wales       99.099980       all        week"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def parse_footfall_data(xls):\n",
    "    sheets =list(xls.keys())[3:]\n",
    "    period_type = \n",
    "    for sheet in sheets:\n",
    "        df = xls.parse(sheet, header = 4).set_index(['Date'])\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "\n",
    "        if 'Week' in sheet:\n",
    "            period_type = 'week'\n",
    "        elif 'Month' in sheet:\n",
    "            period_type = 'month'\n",
    "\n",
    "        if 'region and site' in sheet.lower():\n",
    "            site_type = ...\n",
    "            region ...\n",
    "\n",
    "        else:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df.stack().head().reset_index().rename(columns = {'level_1':'region',0:'footfall_index','Date':'period_end_dt'}).assign(site_type = 'all', period_type = 'week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a1ac40a",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m url = \u001b[33m'\u001b[39m\u001b[33mhttps://www.ons.gov.uk/file?uri=/economy/economicoutputandproductivity/output/datasets/ukretailfootfall/12february2026/ukretailfootfalldataset120226.xlsx\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mparse_footfall_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 111\u001b[39m, in \u001b[36mparse_footfall_data\u001b[39m\u001b[34m(xlsx_path)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_footfall_data\u001b[39m(xlsx_path: Path) -> pd.DataFrame:\n\u001b[32m    107\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[33;03m    Returns a long DataFrame with:\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[33;03m    period_start_dt, period_end_dt, period_type, region, site_type, footfall_index\u001b[39;00m\n\u001b[32m    110\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     xls = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxlsx_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mopenpyxl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m     \u001b[38;5;66;03m# Read sheets\u001b[39;00m\n\u001b[32m    114\u001b[39m     weekly_region      = _read_sheet_auto_header(xls, \u001b[33m\"\u001b[39m\u001b[33m1.Weekly by region\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/pandas/io/excel/_base.py:1567\u001b[39m, in \u001b[36mExcelFile.__init__\u001b[39m\u001b[34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28mself\u001b[39m.engine = engine\n\u001b[32m   1565\u001b[39m \u001b[38;5;28mself\u001b[39m.storage_options = storage_options\n\u001b[32m-> \u001b[39m\u001b[32m1567\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engines\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1568\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_io\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1570\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1571\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/pandas/io/excel/_openpyxl.py:553\u001b[39m, in \u001b[36mOpenpyxlReader.__init__\u001b[39m\u001b[34m(self, filepath_or_buffer, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m    541\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    542\u001b[39m \u001b[33;03mReader using openpyxl engine.\u001b[39;00m\n\u001b[32m    543\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    550\u001b[39m \u001b[33;03m    Arbitrary keyword arguments passed to excel engine.\u001b[39;00m\n\u001b[32m    551\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    552\u001b[39m import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mopenpyxl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/pandas/io/excel/_base.py:563\u001b[39m, in \u001b[36mBaseExcelReader.__init__\u001b[39m\u001b[34m(self, filepath_or_buffer, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = IOHandles(\n\u001b[32m    560\u001b[39m     handle=filepath_or_buffer, compression={\u001b[33m\"\u001b[39m\u001b[33mmethod\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[32m    561\u001b[39m )\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_or_buffer, (ExcelFile, \u001b[38;5;28mself\u001b[39m._workbook_class)):\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m     \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    565\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.handles.handle, \u001b[38;5;28mself\u001b[39m._workbook_class):\n\u001b[32m    568\u001b[39m     \u001b[38;5;28mself\u001b[39m.book = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/pandas/io/common.py:728\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    725\u001b[39m     codecs.lookup_error(errors)\n\u001b[32m    727\u001b[39m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m ioargs = \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    736\u001b[39m handle = ioargs.filepath_or_buffer\n\u001b[32m    737\u001b[39m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/pandas/io/common.py:384\u001b[39m, in \u001b[36m_get_filepath_or_buffer\u001b[39m\u001b[34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[32m    383\u001b[39m req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[32m    385\u001b[39m     content_encoding = req.headers.get(\u001b[33m\"\u001b[39m\u001b[33mContent-Encoding\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding == \u001b[33m\"\u001b[39m\u001b[33mgzip\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/pandas/io/common.py:289\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    283\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    284\u001b[39m \u001b[33;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03mthe stdlib.\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01murllib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrequest\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/urllib/request.py:216\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    215\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/urllib/request.py:525\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process_response.get(protocol, []):\n\u001b[32m    524\u001b[39m     meth = \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m     response = \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/urllib/request.py:634\u001b[39m, in \u001b[36mHTTPErrorProcessor.http_response\u001b[39m\u001b[34m(self, request, response)\u001b[39m\n\u001b[32m    631\u001b[39m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[32m200\u001b[39m <= code < \u001b[32m300\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/urllib/request.py:563\u001b[39m, in \u001b[36mOpenerDirector.error\u001b[39m\u001b[34m(self, proto, *args)\u001b[39m\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[32m    562\u001b[39m     args = (\u001b[38;5;28mdict\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhttp_error_default\u001b[39m\u001b[33m'\u001b[39m) + orig_args\n\u001b[32m--> \u001b[39m\u001b[32m563\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/urllib/request.py:496\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    495\u001b[39m     func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    498\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/urllib/request.py:643\u001b[39m, in \u001b[36mHTTPDefaultErrorHandler.http_error_default\u001b[39m\u001b[34m(self, req, fp, code, msg, hdrs)\u001b[39m\n\u001b[32m    642\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001b[31mHTTPError\u001b[39m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "source": [
    "url = 'https://www.ons.gov.uk/file?uri=/economy/economicoutputandproductivity/output/datasets/ukretailfootfall/12february2026/ukretailfootfalldataset120226.xlsx'\n",
    "\n",
    "parse_footfall_data(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83f7e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "\n",
    "# These are the only site-type labels present in the ONS sheets\n",
    "SITE_TYPES = [\n",
    "    \"District or Local Centre\",\n",
    "    \"Retail Parks\",\n",
    "    \"Town and City Centres\",\n",
    "]\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "\n",
    "def _normalize_table(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given a sheet already loaded to a DataFrame (with unknown intro rows),\n",
    "    find the row that contains 'Date', use that row as the header,\n",
    "    and return the clean table that starts immediately below it.\n",
    "    \"\"\"\n",
    "    raw = df.copy()\n",
    "\n",
    "    # Locate the first occurrence of 'Date' anywhere in the sheet\n",
    "    header_row = None\n",
    "    header_col = None\n",
    "    for i in range(raw.shape[0]):\n",
    "        for j in range(raw.shape[1]):\n",
    "            val = raw.iat[i, j]\n",
    "            if isinstance(val, str) and val.strip() == \"Date\":\n",
    "                header_row, header_col = i, j\n",
    "                break\n",
    "        if header_row is not None:\n",
    "            break\n",
    "    if header_row is None:\n",
    "        raise ValueError(\"Could not find a header row containing 'Date'\")\n",
    "\n",
    "    # Trim columns left of the 'Date' cell (gets rid of notes/blank cols)\n",
    "    trimmed = raw.iloc[:, header_col:].copy()\n",
    "\n",
    "    # Promote the 'Date' row to header and drop rows above it\n",
    "    trimmed.columns = trimmed.iloc[header_row].astype(str).tolist()\n",
    "    trimmed = trimmed.iloc[header_row + 1 :].reset_index(drop=True)\n",
    "\n",
    "    # Drop unnamed/empty columns and fully empty rows\n",
    "    trimmed = trimmed.loc[:, ~trimmed.columns.to_series().str.contains(r\"^Unnamed\", na=False)]\n",
    "    trimmed = trimmed.dropna(how=\"all\")\n",
    "\n",
    "    # Ensure the 'Date' column exists (spelling/case is as in the workbook)\n",
    "    if \"Date\" not in trimmed.columns:\n",
    "        raise ValueError(\"Normalized table does not contain a 'Date' column after header detection\")\n",
    "\n",
    "    return trimmed\n",
    "\n",
    "\n",
    "_site_type_re = re.compile(\n",
    "    r\"(.*)\\s+(District or Local Centre|Retail Parks|Town and City Centres)$\"\n",
    ")\n",
    "\n",
    "def _melt_week_region(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.melt(id_vars=[\"Date\"], var_name=\"region\", value_name=\"footfall_index\")\n",
    "    out[\"site_type\"] = \"all\"\n",
    "    out[\"period_type\"] = \"week\"\n",
    "    return out\n",
    "\n",
    "def _melt_week_site(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.melt(id_vars=[\"Date\"], var_name=\"site_type\", value_name=\"footfall_index\")\n",
    "    out[\"region\"] = \"UK total\"\n",
    "    out[\"period_type\"] = \"week\"\n",
    "    return out\n",
    "\n",
    "def _melt_week_region_site(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    long_df = df.melt(id_vars=[\"Date\"], var_name=\"combo\", value_name=\"footfall_index\")\n",
    "\n",
    "    def split_combo(s):\n",
    "        if pd.isna(s):\n",
    "            return pd.Series({\"region\": None, \"site_type\": None})\n",
    "        s = str(s)\n",
    "        m = _site_type_re.match(s)\n",
    "        if m:\n",
    "            return pd.Series({\"region\": m.group(1).strip(), \"site_type\": m.group(2).strip()})\n",
    "        # Safe fallback if formatting shifts slightly\n",
    "        for st in SITE_TYPES:\n",
    "            if s.endswith(st):\n",
    "                return pd.Series({\"region\": s[: -len(st)].strip(), \"site_type\": st})\n",
    "        return pd.Series({\"region\": s, \"site_type\": None})\n",
    "\n",
    "    parts = long_df[\"combo\"].apply(split_combo)\n",
    "    long_df = pd.concat([long_df.drop(columns=[\"combo\"]), parts], axis=1)\n",
    "    long_df[\"period_type\"] = \"week\"\n",
    "    return long_df\n",
    "\n",
    "def _melt_month_region(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.melt(id_vars=[\"Date\"], var_name=\"region\", value_name=\"footfall_index\")\n",
    "    out[\"site_type\"] = \"all\"\n",
    "    out[\"period_type\"] = \"month\"\n",
    "    return out\n",
    "\n",
    "def _melt_month_site(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.melt(id_vars=[\"Date\"], var_name=\"site_type\", value_name=\"footfall_index\")\n",
    "    out[\"region\"] = \"UK total\"\n",
    "    out[\"period_type\"] = \"month\"\n",
    "    return out\n",
    "\n",
    "def _melt_month_region_site(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    long_df = df.melt(id_vars=[\"Date\"], var_name=\"combo\", value_name=\"footfall_index\")\n",
    "    parts = long_df[\"combo\"].apply(\n",
    "        lambda s: pd.Series(_site_type_re.match(str(s)).groups(), index=[\"region\", \"site_type\"])\n",
    "        if _site_type_re.match(str(s))\n",
    "        else pd.Series({\"region\": str(s), \"site_type\": None})\n",
    "    )\n",
    "    long_df = pd.concat([long_df.drop(columns=[\"combo\"]), parts], axis=1)\n",
    "    long_df[\"period_type\"] = \"month\"\n",
    "    return long_df\n",
    "\n",
    "# ---------- Public function (what youâ€™ll call) ----------\n",
    "\n",
    "def parse_footfall_data(xls: Dict[str, pd.DataFrame], version: str = \"\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    xls : dict[str, DataFrame]\n",
    "        Output of pd.read_excel(..., sheet_name=None). Keys are sheet names.\n",
    "    version : str\n",
    "        Version string (e.g., publication date) to stamp onto every row.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame with columns:\n",
    "      period_start_dt, period_end_dt, period_type, region, site_type, footfall_index, version\n",
    "    \"\"\"\n",
    "\n",
    "    # Helper to find the correct sheet by (partial) name, case-insensitive.\n",
    "    def _pick(name_part: str) -> str:\n",
    "        for k in xls.keys():\n",
    "            if name_part.lower() in str(k).lower():\n",
    "                return k\n",
    "        raise KeyError(f\"Sheet containing '{name_part}' not found in workbook\")\n",
    "\n",
    "    # 1) Normalize each sheet to get a clean table with a 'Date' header.\n",
    "    wk_reg  = _normalize_table(xls[_pick(\"Weekly by region\")])\n",
    "    wk_site = _normalize_table(xls[_pick(\"Weekly by site type\")])\n",
    "    wk_r_s  = _normalize_table(xls[_pick(\"Weekly by region and site\")])\n",
    "\n",
    "    mo_reg  = _normalize_table(xls[_pick(\"Monthly by region\")])\n",
    "    mo_site = _normalize_table(xls[_pick(\"Monthly by site type\")])\n",
    "    mo_r_s  = _normalize_table(xls[_pick(\"Monthly by region and site\")])\n",
    "\n",
    "    # 2) Melt all six into long\n",
    "    w_reg  = _melt_week_region(wk_reg)\n",
    "    w_site = _melt_week_site(wk_site)\n",
    "    w_r_s  = _melt_week_region_site(wk_r_s)\n",
    "\n",
    "    m_reg  = _melt_month_region(mo_reg)\n",
    "    m_site = _melt_month_site(mo_site)\n",
    "    m_r_s  = _melt_month_region_site(mo_r_s)\n",
    "\n",
    "    combined = pd.concat([w_reg, w_site, w_r_s, m_reg, m_site, m_r_s], ignore_index=True)\n",
    "\n",
    "    # 3) Dates: weekly dates are END; monthly dates are START\n",
    "    combined = combined.rename(columns={\"Date\": \"date\"})\n",
    "    combined[\"date\"] = pd.to_datetime(combined[\"date\"], errors=\"coerce\")\n",
    "\n",
    "    combined[\"period_start_dt\"] = combined[\"date\"].where(\n",
    "        combined[\"period_type\"].eq(\"month\"),\n",
    "        combined[\"date\"] - pd.to_timedelta(6, unit=\"D\"),\n",
    "    )\n",
    "    combined[\"period_end_dt\"] = combined[\"date\"].where(\n",
    "        combined[\"period_type\"].eq(\"week\"),\n",
    "        combined[\"date\"],\n",
    "    )\n",
    "    is_month = combined[\"period_type\"].eq(\"month\")\n",
    "    combined.loc[is_month, \"period_end_dt\"] = combined.loc[is_month, \"period_start_dt\"] + pd.offsets.MonthEnd(1)\n",
    "\n",
    "    # 4) Clean up text, types, and add version\n",
    "    combined[\"region\"] = combined[\"region\"].astype(str).str.strip()\n",
    "    combined[\"site_type\"] = combined[\"site_type\"].astype(str).str.strip()\n",
    "    combined.loc[combined[\"region\"].eq(\"nan\"), \"region\"] = None\n",
    "    combined.loc[combined[\"site_type\"].eq(\"nan\"), \"site_type\"] = None\n",
    "\n",
    "    combined[\"footfall_index\"] = pd.to_numeric(combined[\"footfall_index\"], errors=\"coerce\")\n",
    "    combined[\"version\"] = version\n",
    "\n",
    "    out = combined[\n",
    "        [\"period_start_dt\", \"period_end_dt\", \"period_type\", \"region\", \"site_type\", \"footfall_index\", \"version\"]\n",
    "    ].copy()\n",
    "\n",
    "    # Emit dates as date (not datetime)\n",
    "    out[\"period_start_dt\"] = pd.to_datetime(out[\"period_start_dt\"]).dt.date\n",
    "    out[\"period_end_dt\"]   = pd.to_datetime(out[\"period_end_dt\"]).dt.date\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4e2d362",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_footfall_data(xls, version = version)\n",
    "df.to_csv('testdata.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13253ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e89807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd42ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43906d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69190366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e164c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b2b7ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed82a94c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9b7ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266a7cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d77130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68acf4c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7806b700",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26f6f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d4afa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716f67e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7698c0f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbcb59a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2179b00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af3c472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c117ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b60a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaca6dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96755792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5a50b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all sheet names in the Excel file\n",
    "excel_file = pd.ExcelFile('ukretailfootfall.xlsx')\n",
    "print('Available sheets:', excel_file.sheet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ab6067a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most recent dataset link: https://www.ons.gov.uk/file?uri=/economy/economicoutputandproductivity/output/datasets/ukretailfootfall/12february2026/ukretailfootfalldataset120226.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620782bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e83a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d4c431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450d6486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d4ac2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5413339f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f48783d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
